{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import math\n",
    "from datetime import timedelta, datetime\n",
    "\n",
    "# Bad Predictions, OK EVAL Metrics, Work on Prediciton function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lstm_data(df, target_col='Close', sequence_length=60):\n",
    "    \"\"\"\n",
    "    Prepare data for LSTM model by creating sequences\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Input dataframe with features (should have 'date' as index)\n",
    "    target_col (str): Column to predict\n",
    "    sequence_length (int): Number of time steps to use for sequences\n",
    "\n",
    "    Returns:\n",
    "    tuple: (X_train, y_train, X_test, y_test, scalers, feature_columns)\n",
    "           Returns None for any element if preparation fails.\n",
    "    \"\"\"\n",
    "    print(\"\\n==== Preparing Data for LSTM Model ====\")\n",
    "\n",
    "    # Ensure the DataFrame index is a DatetimeIndex\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        print(\"Error: DataFrame index is not a DatetimeIndex. Please set the 'date' column as the index before calling this function.\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "    # Use the globally defined list of features\n",
    "    # Ensure all selected features exist in the dataframe\n",
    "    feature_columns = [col for col in FEATURES_TO_USE if col in df.columns]\n",
    "\n",
    "    # Ensure the target column is always included in the features list\n",
    "    if target_col not in feature_columns:\n",
    "        if target_col in df.columns:\n",
    "             feature_columns.append(target_col)\n",
    "        else:\n",
    "             print(f\"Error: Target column '{target_col}' not found in the DataFrame.\")\n",
    "             return None, None, None, None, None, None\n",
    "\n",
    "    # Check for any specified features that are missing from the DataFrame\n",
    "    missing_features = [col for col in FEATURES_TO_USE if col not in df.columns]\n",
    "    if missing_features:\n",
    "        print(f\"Warning: The following specified features are missing from the DataFrame: {missing_features}\")\n",
    "        # The feature_columns list already excludes these, so no further action needed here\n",
    "\n",
    "    if not feature_columns:\n",
    "         print(\"Error: No valid features selected or found in the DataFrame. Exiting data preparation.\")\n",
    "         return None, None, None, None, None, None\n",
    "\n",
    "\n",
    "    print(f\"Using {len(feature_columns)} features: {feature_columns[:10]}...\") # Print more features for clarity\n",
    "\n",
    "    # Create a new dataframe with selected features, dropping NaN rows\n",
    "    # Only drop NaNs based on the selected feature columns\n",
    "    data = df[feature_columns].dropna()\n",
    "\n",
    "    # Check if enough data remains after dropping NaNs\n",
    "    if len(data) < sequence_length + 1:\n",
    "         print(f\"Error: Not enough data ({len(data)} rows) after dropping NaNs to create sequences of length {sequence_length}.\")\n",
    "         return None, None, None, None, None, None\n",
    "\n",
    "    # Split into training and test sets (70% train, 30% test)\n",
    "    train_size = int(len(data) * 0.7)\n",
    "    train_data = data.iloc[:train_size]\n",
    "    test_data = data.iloc[train_size:]\n",
    "\n",
    "    # Check if train/test sets are large enough to create at least one sequence\n",
    "    if len(train_data) < sequence_length or len(test_data) < sequence_length:\n",
    "        print(f\"Error: Not enough data in train ({len(train_data)}) or test ({len(test_data)}) set after splitting to create sequences of length {sequence_length}.\")\n",
    "        return None, None, None, None, None, None\n",
    "\n",
    "\n",
    "    print(f\"Train data shape: {train_data.shape}, Test data shape: {test_data.shape}\")\n",
    "\n",
    "    # Normalize the data - store scalers for later inverse transformation\n",
    "    scalers = {}\n",
    "    train_scaled = pd.DataFrame(index=train_data.index) # Preserve index\n",
    "    test_scaled = pd.DataFrame(index=test_data.index) # Preserve index\n",
    "\n",
    "    for column in feature_columns:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        # Fit scaler on training data\n",
    "        train_scaled[column] = scaler.fit_transform(train_data[column].values.reshape(-1, 1)).flatten()\n",
    "        # Transform test data with the same scaler\n",
    "        test_scaled[column] = scaler.transform(test_data[column].values.reshape(-1, 1)).flatten()\n",
    "        # Store scaler for inverse transformation later\n",
    "        scalers[column] = scaler\n",
    "\n",
    "    # Create sequences for LSTM\n",
    "    def create_sequences(data, seq_length):\n",
    "        X, y = [], []\n",
    "        # Use .values to work with numpy arrays for sequence creation\n",
    "        data_values = data.values\n",
    "        # Find the index of the target column in the current data_values array\n",
    "        # This is important because the order of columns in 'data' might differ from FEATURES_TO_USE\n",
    "        try:\n",
    "            target_idx_in_values = data.columns.get_loc(target_col)\n",
    "        except KeyError:\n",
    "             print(f\"Error: Target column '{target_col}' not found in the DataFrame slice used for sequence creation.\")\n",
    "             return np.array([]), np.array([]) # Return empty arrays on error\n",
    "\n",
    "\n",
    "        for i in range(len(data_values) - seq_length):\n",
    "            # Extract sequence of features (from i up to i + seq_length - 1)\n",
    "            seq_x = data_values[i:(i + seq_length)]\n",
    "            # Extract target (the target value at the end of the sequence + 1, i.e., at index i + seq_length)\n",
    "            seq_y = data_values[i + seq_length, target_idx_in_values] # Get target from the next time step\n",
    "            X.append(seq_x)\n",
    "            y.append(seq_y)\n",
    "        return np.array(X), np.array(y)\n",
    "\n",
    "    # Generate training sequences\n",
    "    X_train, y_train = create_sequences(train_scaled, sequence_length)\n",
    "    # Generate test sequences\n",
    "    X_test, y_test = create_sequences(test_scaled, sequence_length)\n",
    "\n",
    "    # Check if sequences were created successfully\n",
    "    if X_train.shape[0] == 0 or X_test.shape[0] == 0:\n",
    "         print(f\"Error: Sequence creation failed. X_train shape: {X_train.shape}, X_test shape: {X_test.shape}\")\n",
    "         return None, None, None, None, None, None\n",
    "\n",
    "\n",
    "    print(f\"Training sequences shape: {X_train.shape}\")\n",
    "    print(f\"Test sequences shape: {X_test.shape}\")\n",
    "\n",
    "    return X_train, y_train, X_test, y_test, scalers, feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test, scalers, target_col='Close'):\n",
    "    \"\"\"\n",
    "    Evaluate the LSTM model performance\n",
    "\n",
    "    Parameters:\n",
    "    model: Trained LSTM model\n",
    "    X_train, y_train, X_test, y_test: Training and test data\n",
    "    scalers (dict): Dictionary of scalers for each feature\n",
    "    target_col (str): Target column name\n",
    "\n",
    "    Returns:\n",
    "    tuple: (train_predictions, test_predictions, metrics)\n",
    "           Returns None for any element if evaluation fails.\n",
    "    \"\"\"\n",
    "    print(\"\\n==== Evaluating Model Performance ====\")\n",
    "\n",
    "    if model is None:\n",
    "        print(\"Error: Model is not trained. Cannot evaluate.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Get target scaler for inverse transformation\n",
    "    if target_col not in scalers:\n",
    "        print(f\"Error: Scaler for target column '{target_col}' not found.\")\n",
    "        return None, None, None\n",
    "    scaler = scalers[target_col]\n",
    "\n",
    "    # Make predictions\n",
    "    train_predictions_scaled = model.predict(X_train)\n",
    "    test_predictions_scaled = model.predict(X_test)\n",
    "\n",
    "    # Inverse transform predictions and actual values to original scale\n",
    "    train_predictions = scaler.inverse_transform(train_predictions_scaled)\n",
    "    test_predictions = scaler.inverse_transform(test_predictions_scaled)\n",
    "\n",
    "    # Transform the original y values back to original scale\n",
    "    y_train_orig = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "    y_test_orig = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "    # Calculate metrics - make sure arrays are the same length\n",
    "    # Ensure shapes match before calculating metrics\n",
    "    if y_train_orig.shape != train_predictions.shape:\n",
    "         print(f\"Warning: Train actual ({y_train_orig.shape}) and prediction ({train_predictions.shape}) shapes mismatch. Truncating.\")\n",
    "         min_len = min(y_train_orig.shape[0], train_predictions.shape[0])\n",
    "         y_train_orig = y_train_orig[:min_len]\n",
    "         train_predictions = train_predictions[:min_len]\n",
    "\n",
    "    if y_test_orig.shape != test_predictions.shape:\n",
    "         print(f\"Warning: Test actual ({y_test_orig.shape}) and prediction ({test_predictions.shape}) shapes mismatch. Truncating.\")\n",
    "         min_len = min(y_test_orig.shape[0], test_predictions.shape[0])\n",
    "         y_test_orig = y_test_orig[:min_len]\n",
    "         test_predictions = test_predictions[:min_len]\n",
    "\n",
    "\n",
    "    train_rmse = math.sqrt(mean_squared_error(y_train_orig, train_predictions))\n",
    "    test_rmse = math.sqrt(mean_squared_error(y_test_orig, test_predictions))\n",
    "\n",
    "    train_mae = mean_absolute_error(y_train_orig, train_predictions)\n",
    "    test_mae = mean_absolute_error(y_test_orig, test_predictions)\n",
    "\n",
    "    train_r2 = r2_score(y_train_orig, train_predictions)\n",
    "    test_r2 = r2_score(y_test_orig, test_predictions)\n",
    "\n",
    "    # Store metrics in dictionary\n",
    "    metrics = {\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'test_mae': test_mae,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2\n",
    "    }\n",
    "\n",
    "    # Print metrics\n",
    "    print(f\"Training RMSE: {train_rmse:.2f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.2f}\")\n",
    "    print(f\"Training MAE: {train_mae:.2f}\")\n",
    "    print(f\"Test MAE: {test_mae:.2f}\")\n",
    "    print(f\"Training R²: {train_r2:.4f}\")\n",
    "    print(f\"Test R²: {test_r2:.4f}\")\n",
    "\n",
    "    return train_predictions, test_predictions, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(df, train_predictions, test_predictions, future_predictions=None, future_dates=None, sequence_length=60, target_col='Close', history=None):\n",
    "    \"\"\"\n",
    "    Visualize model predictions against actual prices, including future predictions\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Original dataframe (before sequence creation/dropping NaNs, with DatetimeIndex)\n",
    "    train_predictions, test_predictions: Model predictions (original scale)\n",
    "    future_predictions (array): Predictions for future dates (original scale)\n",
    "    future_dates (array): Future dates for predictions\n",
    "    sequence_length (int): Sequence length used for LSTM\n",
    "    target_col (str): Target column name\n",
    "    history: Training history\n",
    "    \"\"\"\n",
    "    print(\"\\n==== Visualizing Predictions ====\")\n",
    "\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        print(\"Error: DataFrame index is not a DatetimeIndex. Cannot visualize.\")\n",
    "        return\n",
    "\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    # Plot actual data - use the original dataframe index\n",
    "    plt.plot(df.index, df[target_col], color='blue', label=f'Actual {target_col} Price')\n",
    "\n",
    "    # Calculate indices for predictions\n",
    "    # We need the index of the data used for training and testing *after* dropping NaNs\n",
    "    # Find the first non-NaN row in the original dataframe for the selected features\n",
    "    # Use the global FEATURES_TO_USE list to determine which columns were used\n",
    "    data_for_plotting = df[FEATURES_TO_USE].dropna()\n",
    "    first_valid_index = data_for_plotting.index[0]\n",
    "\n",
    "    # Find the split point index in the data_for_plotting index\n",
    "    train_size_after_dropna = int(len(data_for_plotting) * 0.7)\n",
    "\n",
    "    # The index where training predictions start (after the first sequence)\n",
    "    # The training predictions correspond to the dates from sequence_length to train_size_after_dropna - 1\n",
    "    if len(data_for_plotting) > sequence_length:\n",
    "        train_predictions_start_index = data_for_plotting.index[sequence_length]\n",
    "        train_predictions_end_index = data_for_plotting.index[train_size_after_dropna -1] # Last date of y_train\n",
    "    else:\n",
    "        train_predictions_start_index = None\n",
    "        train_predictions_end_index = None\n",
    "\n",
    "\n",
    "    # The index where test predictions start\n",
    "    # The test predictions correspond to the dates from train_size_after_dropna + sequence_length to end\n",
    "    if len(data_for_plotting) > train_size_after_dropna + sequence_length:\n",
    "         test_predictions_start_index = data_for_plotting.index[train_size_after_dropna + sequence_length]\n",
    "         test_predictions_end_index = data_for_plotting.index[-1] # Last date of y_test\n",
    "    else:\n",
    "         test_predictions_start_index = None\n",
    "         test_predictions_end_index = None\n",
    "\n",
    "\n",
    "    # Get the actual date indices for plotting\n",
    "    train_plot_dates = pd.Index([]) # Initialize as empty\n",
    "    if train_predictions_start_index is not None and train_predictions_end_index is not None:\n",
    "         train_plot_dates = data_for_plotting.loc[train_predictions_start_index:train_predictions_end_index].index\n",
    "\n",
    "\n",
    "    test_plot_dates = pd.Index([]) # Initialize as empty\n",
    "    if test_predictions_start_index is not None and test_predictions_end_index is not None:\n",
    "         test_plot_dates = data_for_plotting.loc[test_predictions_start_index:test_predictions_end_index].index\n",
    "\n",
    "\n",
    "    # Plot training predictions\n",
    "    if len(train_plot_dates) > 0 and train_predictions is not None and len(train_predictions) > 0:\n",
    "        # Ensure lengths match\n",
    "        min_len = min(len(train_plot_dates), len(train_predictions))\n",
    "        plt.plot(train_plot_dates[:min_len], train_predictions[:min_len].flatten(),\n",
    "                 color='green', label='Training Predictions')\n",
    "    else:\n",
    "        print(\"Warning: Not enough data or predictions to plot training predictions.\")\n",
    "\n",
    "\n",
    "    # Plot test predictions\n",
    "    if len(test_plot_dates) > 0 and test_predictions is not None and len(test_predictions) > 0:\n",
    "         # Ensure lengths match\n",
    "        min_len = min(len(test_plot_dates), len(test_predictions))\n",
    "        plt.plot(test_plot_dates[:min_len], test_predictions[:min_len].flatten(),\n",
    "                 color='red', label='Test Predictions')\n",
    "    else:\n",
    "        print(\"Warning: Not enough data or predictions to plot test predictions.\")\n",
    "\n",
    "\n",
    "    # Plot future predictions if available\n",
    "    if future_predictions is not None and future_dates is not None and len(future_dates) > 0 and len(future_predictions) > 0:\n",
    "        plt.plot(future_dates, future_predictions.flatten(), color='purple', linestyle='--', linewidth=2,\n",
    "                 label='Future Predictions')\n",
    "    else:\n",
    "        print(\"Warning: Future predictions or dates not available/empty. Skipping future prediction plot.\")\n",
    "\n",
    "\n",
    "    # Use the global stock ticker and title for chart titles\n",
    "    stock_ticker = GLOBAL_STOCK_TICKER\n",
    "    base_title = GLOBAL_STOCK_TITLE\n",
    "\n",
    "    plt.title(f'{stock_ticker} {target_col} Price Prediction')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(f'{target_col} Price ($)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Format x-axis dates\n",
    "    plt.gca().xaxis.set_major_formatter(mdates.DateFormatter('%Y-%m'))\n",
    "    plt.gca().xaxis.set_major_locator(mdates.MonthLocator(interval=6)) # Adjusted interval for better readability\n",
    "    plt.gcf().autofmt_xdate()\n",
    "\n",
    "    # Save the prediction chart with the ticker in the filename\n",
    "    plt.savefig(f'{stock_ticker}_stock_prediction_with_future.png')\n",
    "    plt.show()\n",
    "\n",
    "    # Plot loss during training if history is available\n",
    "    if history is not None:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title(f'{stock_ticker} Model Loss During Training') # Add ticker to title\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss (MSE)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "\n",
    "        # Save the loss chart with the ticker in the filename\n",
    "        plt.savefig(f'{stock_ticker}_training_loss.png')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_future(model, df, scalers, feature_columns, target_col='Close', sequence_length=60, days_to_predict=365):\n",
    "    \"\"\"\n",
    "    Predict future values using the trained LSTM model\n",
    "\n",
    "    Parameters:\n",
    "    model: Trained LSTM model\n",
    "    df (DataFrame): Original dataframe with features (before sequence creation/dropping NaNs, with DatetimeIndex)\n",
    "    scalers (dict): Dictionary of scalers for each feature\n",
    "    feature_columns (list): List of feature column names used in the model\n",
    "    target_col (str): Target column name to predict\n",
    "    sequence_length (int): Number of time steps for sequences\n",
    "    days_to_predict (int): Number of days to predict into the future\n",
    "\n",
    "    Returns:\n",
    "    tuple: (future_predictions, future_dates)\n",
    "           Returns None for any element if prediction fails.\n",
    "    \"\"\"\n",
    "    print(f\"\\n==== Predicting {days_to_predict} Days into the Future for {GLOBAL_STOCK_TICKER} ====\")\n",
    "\n",
    "    if model is None:\n",
    "        print(\"Error: Model is not trained. Cannot predict future.\")\n",
    "        return None, None\n",
    "\n",
    "    # Use the data that was actually used for training/testing (after dropping NaNs)\n",
    "    # Ensure these columns exist in the original df\n",
    "    valid_feature_columns = [col for col in feature_columns if col in df.columns]\n",
    "    if not valid_feature_columns:\n",
    "        print(\"Error: None of the specified feature columns found in the input DataFrame for future prediction.\")\n",
    "        return None, None\n",
    "\n",
    "    data_for_prediction = df[valid_feature_columns].dropna()\n",
    "\n",
    "    # Check if enough data remains after dropping NaNs\n",
    "    if len(data_for_prediction) < sequence_length:\n",
    "        print(f\"Error: Not enough data ({len(data_for_prediction)} rows) to create the initial sequence of length {sequence_length} for future prediction.\")\n",
    "        return None, None\n",
    "\n",
    "    # Create a copy of the last sequence_length records from the cleaned data\n",
    "    latest_data = data_for_prediction.tail(sequence_length).copy()\n",
    "\n",
    "    # Scale the data using the saved scalers\n",
    "    latest_data_scaled = pd.DataFrame()\n",
    "    for column in valid_feature_columns: # Use valid_feature_columns here\n",
    "        if column in scalers:\n",
    "            scaler = scalers[column]\n",
    "            latest_data_scaled[column] = scaler.transform(latest_data[column].values.reshape(-1, 1)).flatten()\n",
    "        else:\n",
    "            print(f\"Warning: Scaler for column '{column}' not found. Skipping scaling for this column in future prediction.\")\n",
    "            # If a scaler is missing, the prediction might be inaccurate.\n",
    "            # Depending on the feature, you might need a different strategy.\n",
    "            # For now, we'll just use the original value, which might not be ideal.\n",
    "            latest_data_scaled[column] = latest_data[column].values.flatten()\n",
    "\n",
    "\n",
    "    # Get target scaler for inverse transform\n",
    "    if target_col not in scalers:\n",
    "        print(f\"Error: Scaler for target column '{target_col}' not found for inverse transformation.\")\n",
    "        return None, None\n",
    "    target_scaler = scalers[target_col]\n",
    "\n",
    "    # Generate future dates - start from the day *after* the last date in the original data\n",
    "    if not df.empty:\n",
    "        last_date_in_data = df.index[-1]\n",
    "        future_dates = [last_date_in_data + timedelta(days=i+1) for i in range(days_to_predict)]\n",
    "    else:\n",
    "        print(\"Error: Original DataFrame is empty. Cannot generate future dates.\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "    # Make future predictions one day at a time\n",
    "    future_predictions = []\n",
    "\n",
    "    # Create a copy of the latest scaled data for iterative prediction\n",
    "    current_sequence_scaled = latest_data_scaled.values.copy()\n",
    "\n",
    "    # Find the index of the target column in the valid_feature_columns list\n",
    "    try:\n",
    "        target_idx_in_features = valid_feature_columns.index(target_col)\n",
    "    except ValueError:\n",
    "        print(f\"Error: Target column '{target_col}' not found in valid_feature_columns list.\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "    for i in range(days_to_predict):\n",
    "        # Reshape for LSTM model (samples, time steps, features)\n",
    "        X_pred = current_sequence_scaled.reshape(1, sequence_length, len(valid_feature_columns)) # Use length of valid features\n",
    "\n",
    "        # Predict next day (scaled)\n",
    "        next_day_scaled = model.predict(X_pred, verbose=0) # Set verbose to 0 to reduce output during prediction loop\n",
    "\n",
    "        # Inverse transform to get actual price\n",
    "        next_day_price = target_scaler.inverse_transform(next_day_scaled)\n",
    "        future_predictions.append(next_day_price[0, 0])\n",
    "\n",
    "        # For the next iteration, update the sequence by removing the first row and adding the new prediction\n",
    "        # Create a new row with all features for the predicted day\n",
    "        new_row_scaled = np.zeros(len(valid_feature_columns)) # Use length of valid features\n",
    "\n",
    "        # Set the target value (scaled) to the predicted scaled value\n",
    "        new_row_scaled[target_idx_in_features] = next_day_scaled[0, 0]\n",
    "\n",
    "        # For other features, use the scaled values from the last known data point in the sequence\n",
    "        # A more advanced approach might involve predicting other features or using external data\n",
    "        for j, col in enumerate(valid_feature_columns): # Iterate through valid features\n",
    "            if col != target_col:\n",
    "                new_row_scaled[j] = current_sequence_scaled[-1, j]\n",
    "\n",
    "        # Update the sequence: remove first row and add new prediction row\n",
    "        current_sequence_scaled = np.vstack([current_sequence_scaled[1:], new_row_scaled])\n",
    "\n",
    "    # Convert future predictions to numpy array\n",
    "    future_predictions = np.array(future_predictions).reshape(-1, 1)\n",
    "\n",
    "    # Create a dataframe with future predictions\n",
    "    future_df = pd.DataFrame({\n",
    "        'Date': future_dates,\n",
    "        f'Predicted_{target_col}': future_predictions.flatten()\n",
    "    })\n",
    "    future_df.set_index('Date', inplace=True)\n",
    "\n",
    "    # Save future predictions to CSV with the ticker in the filename\n",
    "    output_filename = f'{GLOBAL_STOCK_TICKER}_future_predictions.csv'\n",
    "    try:\n",
    "        future_df.to_csv(output_filename)\n",
    "        print(f\"Future predictions saved to '{output_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving future predictions to {output_filename}: {e}\")\n",
    "\n",
    "\n",
    "    return future_predictions, future_dates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lstm(df, target_col='Close', sequence_length=60, epochs=50, batch_size=32, predict_days=365):\n",
    "    \"\"\"\n",
    "    Main function to run LSTM prediction on stock data and predict future values\n",
    "\n",
    "    Parameters:\n",
    "    df (DataFrame): Processed dataframe with features (should have 'date' as index)\n",
    "    target_col (str): Column to predict (default: 'Close')\n",
    "    sequence_length (int): Number of time steps for sequences (default: 60)\n",
    "    epochs (int): Maximum training epochs (default: 50)\n",
    "    batch_size (int): Training batch size (default: 32)\n",
    "    predict_days (int): Number of days to predict into the future (default: 365)\n",
    "\n",
    "    Returns:\n",
    "    tuple: (model, metrics, predictions, future_predictions, future_dates)\n",
    "           Returns None for any element if a critical error occurs.\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrame index is a DatetimeIndex before proceeding\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        print(\"Error: DataFrame index is not a DatetimeIndex. Please set the 'date' column as the index.\")\n",
    "        return None, None, None, None, None\n",
    "\n",
    "\n",
    "    # Prepare data\n",
    "    X_train, y_train, X_test, y_test, scalers, feature_columns = prepare_lstm_data(\n",
    "        df, target_col=target_col, sequence_length=sequence_length\n",
    "    )\n",
    "\n",
    "    # Check if data preparation was successful\n",
    "    if X_train is None or y_train is None or X_test is None or y_test is None or scalers is None or feature_columns is None:\n",
    "         print(\"Data preparation failed. Exiting prediction process.\")\n",
    "         return None, None, None, None, None\n",
    "\n",
    "\n",
    "    # Train model\n",
    "    model, history = train_lstm_model(\n",
    "        X_train, y_train, X_test, y_test,\n",
    "        epochs=epochs, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Check if model training was successful\n",
    "    if model is None:\n",
    "         print(\"Model training failed. Exiting prediction process.\")\n",
    "         return None, None, None, None, None\n",
    "\n",
    "\n",
    "    # Evaluate model\n",
    "    train_predictions, test_predictions, metrics = evaluate_model(\n",
    "        model, X_train, y_train, X_test, y_test, scalers, target_col\n",
    "    )\n",
    "\n",
    "    # Check if evaluation was successful\n",
    "    if train_predictions is None or test_predictions is None or metrics is None:\n",
    "         print(\"Model evaluation failed. Exiting prediction process.\")\n",
    "         return model, None, None, None, None # Return model if trained, but no metrics/predictions\n",
    "\n",
    "\n",
    "    # Predict future values\n",
    "    future_predictions, future_dates = predict_future(\n",
    "        model, df, scalers, feature_columns, # Pass feature_columns from prepare_lstm_data\n",
    "        target_col=target_col, sequence_length=sequence_length, days_to_predict=predict_days\n",
    "    )\n",
    "\n",
    "    # Visualize results including future predictions\n",
    "    # Pass the original DataFrame 'df' for correct date indexing in visualization\n",
    "    visualize_predictions(\n",
    "        df, train_predictions, test_predictions,\n",
    "        future_predictions, future_dates,\n",
    "        sequence_length, target_col, history\n",
    "    )\n",
    "\n",
    "    # Save model with the ticker in the filename\n",
    "    model_filename = f'{GLOBAL_STOCK_TICKER}_lstm_model.h5'\n",
    "    try:\n",
    "        model.save(model_filename)\n",
    "        print(f\"Model saved as '{model_filename}'\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving model to {model_filename}: {e}\")\n",
    "\n",
    "\n",
    "    # Create a combined dataset with historical and future predictions\n",
    "    # Use the original dataframe's index for historical data\n",
    "    historical_actual = df[[target_col]].copy()\n",
    "    historical_actual.columns = [f'Actual_{target_col}'] # Use target_col in column name\n",
    "\n",
    "    # Create a dataframe for future predictions\n",
    "    if future_predictions is not None and future_dates is not None:\n",
    "        future_df = pd.DataFrame({\n",
    "            f'Predicted_{target_col}': future_predictions.flatten() # Use target_col in column name\n",
    "        }, index=future_dates)\n",
    "\n",
    "        # Combine historical and future data\n",
    "        combined_df = pd.concat([historical_actual, future_df])\n",
    "        combined_output_filename = f'{GLOBAL_STOCK_TICKER}_historical_and_future.csv'\n",
    "        try:\n",
    "            combined_df.to_csv(combined_output_filename)\n",
    "            print(f\"Combined historical and future predictions saved to '{combined_output_filename}'\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving combined data to {combined_output_filename}: {e}\")\n",
    "    else:\n",
    "        print(\"Skipping saving combined historical and future data due to failed future prediction.\")\n",
    "        combined_df = historical_actual # Only historical data available\n",
    "\n",
    "\n",
    "    return model, metrics, test_predictions, future_predictions, future_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load processed data for LSTM prediction from: ML_Final_Project/Lusofona/merged_dataset_AAPL.csv\n",
      "Error: Merged data file not found at ML_Final_Project/Lusofona/merged_dataset_AAPL.csv.\n",
      "Please ensure you have run the EDA script first to generate this file.\n",
      "\n",
      "Skipping LSTM prediction due to data loading or preparation errors for AAPL.\n"
     ]
    }
   ],
   "source": [
    "# Example usage with the processed dataframe\n",
    "if __name__ == \"__main__\":\n",
    "    # --- Update file paths and global variables here ---\n",
    "    # Set the global ticker and title\n",
    "    GLOBAL_STOCK_TICKER = \"AAPL\" # Example: Change to \"AAPL\" for Apple\n",
    "    GLOBAL_STOCK_TITLE = \"Stock Analysis\" # Can keep generic or make specific\n",
    "\n",
    "    # Define the list of features to use for the LSTM model\n",
    "    # You can easily add or remove features from this list\n",
    "    # Ensure 'date' is NOT in this list as it's used as the index\n",
    "    # Ensure the target column ('Close' by default) is also included in this list\n",
    "    FEATURES_TO_USE = [\n",
    "        'Open', 'High', 'Low', 'Close', 'Volume', # Core stock prices/volume\n",
    "        'revenue', 'netIncome',    # Key financial metrics\n",
    "        'eps',\n",
    "        'grossProfitRatio', 'operatingIncomeRatio', 'netIncomeRatio',\n",
    "        'weightedAverageShsOutDil', # Needed for price-to-sales\n",
    "        'days_since_financial_update', # Days since last financial report\n",
    "        'MA_50', 'MA_200', # Longer-term Moving Averages\n",
    "        'RSI', # Relative Strength Index\n",
    "        'volatility_20d', # Longer-term Volatility\n",
    "        'PE_ratio', # Price-to-Earnings\n",
    "        'volume_ma_5', # Volume metrics\n",
    "        'price_to_sales', # Price-to-Sales\n",
    "        # Removed: 'calendarYear', 'grossProfit', 'operatingIncome', 'epsdiluted',\n",
    "        # 'MA_5', 'MA_10', 'MA_20', 'volatility_5d', 'volatility_10d', 'volume_change',\n",
    "        # 'price_change_pct', 'price_change', 'returns', 'momentum_5d', 'momentum_10d', 'momentum_20d'\n",
    "    ]\n",
    "\n",
    "\n",
    "    # --- Load the already processed merged dataset ---\n",
    "    # Make sure you have run the EDA script first to generate this file.\n",
    "    # The filename is expected to include the ticker based on the EDA script's saving logic.\n",
    "    merged_data_path = f\"merged_dataset_{GLOBAL_STOCK_TICKER}.csv\"\n",
    "\n",
    "\n",
    "    print(f\"Attempting to load processed data for LSTM prediction from: {merged_data_path}\")\n",
    "\n",
    "    try:\n",
    "        # --- MODIFICATION START ---\n",
    "        # Explicitly specify CSV reading parameters\n",
    "        enhanced_df = pd.read_csv(\n",
    "            merged_data_path,\n",
    "            sep=',',           # Delimiter is comma\n",
    "            encoding='utf-8',  # Standard encoding\n",
    "            decimal='.',       # Decimal separator is a dot\n",
    "            header=0,          # Header is in the first row (index 0)\n",
    "            index_col='date',  # Use the 'date' column as the index\n",
    "            parse_dates=True   # Automatically parse the index as dates\n",
    "        )\n",
    "        # --- MODIFICATION END ---\n",
    "\n",
    "        # Check if the index is a DatetimeIndex after loading\n",
    "        if isinstance(enhanced_df.index, pd.DatetimeIndex):\n",
    "            print(\"Data loaded successfully and 'date' column set as DatetimeIndex.\")\n",
    "            print(f\"Loaded data shape: {enhanced_df.shape}\")\n",
    "            print(f\"Loaded data columns: {enhanced_df.columns.tolist()}\")\n",
    "            print(f\"Loaded data index type: {type(enhanced_df.index)}\")\n",
    "        else:\n",
    "             print(\"Error: 'date' column was not successfully set as a DatetimeIndex during loading.\")\n",
    "             # Fallback: try converting after loading if index_col didn't work as expected\n",
    "             if 'date' in enhanced_df.columns:\n",
    "                  enhanced_df['date'] = pd.to_datetime(enhanced_df['date'])\n",
    "                  enhanced_df.set_index('date', inplace=True)\n",
    "                  if isinstance(enhanced_df.index, pd.DatetimeIndex):\n",
    "                       print(\"Successfully set 'date' column as DatetimeIndex after loading.\")\n",
    "                       print(f\"Loaded data shape: {enhanced_df.shape}\")\n",
    "                       print(f\"Loaded data columns: {enhanced_df.columns.tolist()}\")\n",
    "                       print(f\"Loaded data index type: {type(enhanced_df.index)}\")\n",
    "                  else:\n",
    "                       print(\"Critical Error: Could not set 'date' column as DatetimeIndex. Cannot proceed with LSTM.\")\n",
    "                       enhanced_df = None # Set to None to indicate failure\n",
    "             else:\n",
    "                  print(\"Critical Error: 'date' column not found in the loaded CSV. Cannot proceed with LSTM.\")\n",
    "                  enhanced_df = None # Set to None to indicate failure\n",
    "\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Merged data file not found at {merged_data_path}.\")\n",
    "        print(\"Please ensure you have run the EDA script first to generate this file.\")\n",
    "        enhanced_df = None # Set to None to indicate failure\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the merged data: {e}\")\n",
    "        # Print traceback for more detailed error information\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        enhanced_df = None # Set to None to indicate failure\n",
    "\n",
    "\n",
    "    if enhanced_df is not None and not enhanced_df.empty:\n",
    "        print(\"\\nProcessed data loaded successfully. Running LSTM prediction.\")\n",
    "\n",
    "        # Run LSTM prediction with future forecasting\n",
    "        # Adjust sequence_length, epochs, batch_size, predict_days as needed\n",
    "        model, metrics, test_predictions, future_predictions, future_dates = predict_lstm(\n",
    "            enhanced_df,\n",
    "            target_col='Close',\n",
    "            sequence_length=60, # Number of past days to look at for prediction\n",
    "            epochs=50, # Maximum training epochs\n",
    "            batch_size=32,\n",
    "            predict_days=30 # Number of future days to predict\n",
    "        )\n",
    "\n",
    "        if metrics is not None:\n",
    "            print(\"\\n==== Prediction Complete ====\")\n",
    "            print(f\"Model metrics for {GLOBAL_STOCK_TICKER}: {metrics}\")\n",
    "            if test_predictions is not None:\n",
    "                 print(f\"Test predictions shape: {test_predictions.shape}\")\n",
    "            if future_predictions is not None:\n",
    "                 print(f\"Future predictions shape: {future_predictions.shape}\")\n",
    "                 print(f\"Predicted {len(future_dates)} days into the future\")\n",
    "        else:\n",
    "             print(f\"\\nLSTM prediction process failed for {GLOBAL_STOCK_TICKER}.\")\n",
    "\n",
    "    else:\n",
    "        print(f\"\\nSkipping LSTM prediction due to data loading or preparation errors for {GLOBAL_STOCK_TICKER}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Potential changes\n",
    "\n",
    "\n",
    "# %%\n",
    "# Potential improvements (suggestions, NOT implemented in the code above):\n",
    "\n",
    "# 1. Hyperparameter Tuning:\n",
    "#    - Use techniques like Grid Search or Random Search with cross-validation\n",
    "#      to find optimal values for sequence_length, LSTM units, dropout rates,\n",
    "#      batch size, and learning rate.\n",
    "\n",
    "# 2. Feature Engineering:\n",
    "#    - Explore other potentially useful features:\n",
    "#      - Lagged financial metrics (e.g., revenue from 1 quarter ago)\n",
    "#      - Macroeconomic indicators (interest rates, inflation)\n",
    "#      - Sentiment analysis scores from news or social media\n",
    "#      - Industry-specific metrics\n",
    "#    - Consider transforming features (e.g., using log returns as suggested,\n",
    "#      or applying other transformations like differencing).\n",
    "\n",
    "# 3. Model Architecture:\n",
    "#    - Experiment with different LSTM architectures:\n",
    "#      - More or fewer LSTM layers\n",
    "#      - Bidirectional LSTMs\n",
    "#      - Adding other layers like Convolutional layers (CNN-LSTM) for pattern recognition\n",
    "#      - Attention mechanisms to focus on important time steps\n",
    "\n",
    "# 4. Time Series Cross-Validation:\n",
    "#    - Instead of a simple train/test split, use time series cross-validation\n",
    "#      (e.g., expanding window or sliding window) to get a more robust estimate\n",
    "#      of model performance on unseen future data.\n",
    "\n",
    "# 5. Ensemble Methods:\n",
    "#    - Train multiple LSTM models with different architectures or features\n",
    "#      and combine their predictions.\n",
    "\n",
    "# 6. Handling Financial Data Frequency:\n",
    "#    - The current approach forward-fills financial data. Consider alternative ways\n",
    "#      to incorporate lower-frequency financial data into high-frequency stock data,\n",
    "#      such as using financial data as static features for the period, or using\n",
    "#      more sophisticated merging techniques.\n",
    "\n",
    "# 7. Prediction Strategy:\n",
    "#    - The current future prediction is iterative (predict one day, add to sequence, predict next).\n",
    "#      This can suffer from accumulating errors. Explore multi-step forecasting strategies\n",
    "#      where the model predicts multiple future steps at once.\n",
    "\n",
    "# 8. Robust Scaling:\n",
    "#    - While MinMaxScaler is common, consider using RobustScaler if your data\n",
    "#      contains significant outliers.\n",
    "\n",
    "# 9. Incorporate External Events:\n",
    "#    - Major news events, earnings announcements, or market shocks can significantly\n",
    "#      impact stock prices. Incorporating these as external features or using event-aware\n",
    "#      models could improve predictions.\n",
    "\n",
    "# 10. Different Loss Functions:\n",
    "#     - Experiment with different loss functions beyond MSE, such as Mean Absolute Error (MAE)\n",
    "#       or Huber loss, which might be less sensitive to outliers.\n",
    "\n",
    "\n",
    "\n",
    "# 1. Modify the LSTM architecture to be less complex\n",
    "def build_lstm_model(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(units=32, return_sequences=True, input_shape=input_shape, \n",
    "                  kernel_regularizer=tf.keras.regularizers.l2(0.001)))  # Add L2 regularization\n",
    "    model.add(Dropout(0.3))  # Increase dropout\n",
    "    model.add(LSTM(units=32, return_sequences=False, \n",
    "                  kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(units=16))\n",
    "    model.add(Dense(units=1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "    # Original Model\n",
    "def build_lstm_model(input_shape):\n",
    "    \"\"\"\n",
    "    Build LSTM model architecture\n",
    "    \n",
    "    Parameters:\n",
    "    input_shape (tuple): Shape of input data (sequence_length, features)\n",
    "    \n",
    "    Returns:\n",
    "    model: Compiled LSTM model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer with return sequences for stacking\n",
    "    model.add(LSTM(units=50, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Second LSTM layer\n",
    "    model.add(LSTM(units=50, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Dense layers for prediction\n",
    "    model.add(Dense(units=25))\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "# 2. Use log returns instead of raw prices\n",
    "def prepare_log_returns(df, target_col='Close'):\n",
    "    df['log_price'] = np.log(df[target_col])\n",
    "    df['log_return'] = df['log_price'].diff()\n",
    "    return df\n",
    "\n",
    "# 3. More aggressive early stopping\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
