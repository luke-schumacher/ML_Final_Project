{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_lstm_data(df, target_col='Close', sequence_length=60):\n",
    "    \"\"\"\n",
    "    Prepare data for LSTM model by creating sequences\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Input dataframe with features\n",
    "    target_col (str): Column to predict\n",
    "    sequence_length (int): Number of time steps to use for sequences\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (X_train, y_train, X_test, y_test, scalers)\n",
    "    \"\"\"\n",
    "    print(\"\\n==== Preparing Data for LSTM Model ====\")\n",
    "    \n",
    "    # Get list of features to use (numeric columns)\n",
    "    feature_columns = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    \n",
    "    # Remove any derived features that use future information to avoid data leakage\n",
    "    features_to_exclude = [\n",
    "        'price_change_pct', 'price_change', 'returns',  # These are based on future Close prices\n",
    "        'momentum_5d', 'momentum_10d', 'momentum_20d'   # These look ahead in time\n",
    "    ]\n",
    "    \n",
    "    feature_columns = [col for col in feature_columns if col not in features_to_exclude]\n",
    "    \n",
    "    # Make sure target is in features\n",
    "    if target_col not in feature_columns:\n",
    "        feature_columns.append(target_col)\n",
    "    \n",
    "    print(f\"Using {len(feature_columns)} features: {feature_columns[:5]}...\")\n",
    "    \n",
    "    # Create a new dataframe with selected features, dropping NaN rows\n",
    "    data = df[feature_columns].dropna()\n",
    "    \n",
    "    # Split into training and test sets (70% train, 30% test)\n",
    "    train_size = int(len(data) * 0.7)\n",
    "    train_data = data.iloc[:train_size]\n",
    "    test_data = data.iloc[train_size:]\n",
    "    \n",
    "    print(f\"Train data shape: {train_data.shape}, Test data shape: {test_data.shape}\")\n",
    "    \n",
    "    # Normalize the data - store scalers for later inverse transformation\n",
    "    scalers = {}\n",
    "    train_scaled = pd.DataFrame()\n",
    "    test_scaled = pd.DataFrame()\n",
    "    \n",
    "    for column in feature_columns:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "        # Fit scaler on training data\n",
    "        train_scaled[column] = scaler.fit_transform(train_data[column].values.reshape(-1, 1)).flatten()\n",
    "        # Transform test data with the same scaler\n",
    "        test_scaled[column] = scaler.transform(test_data[column].values.reshape(-1, 1)).flatten()\n",
    "        # Store scaler for inverse transformation later\n",
    "        scalers[column] = scaler\n",
    "\n",
    "            # Create sequences for LSTM\n",
    "    def create_sequences(data, seq_length):\n",
    "        X, y = [], []\n",
    "        for i in range(len(data) - seq_length):\n",
    "            # Extract sequence of features\n",
    "            seq_x = data.iloc[i:(i + seq_length)].values\n",
    "            # Extract target\n",
    "            seq_y = data.iloc[i + seq_length][target_col]\n",
    "            X.append(seq_x)\n",
    "            y.append(seq_y)\n",
    "        return np.array(X), np.array(y)\n",
    "    \n",
    "    # Generate training sequences\n",
    "    X_train, y_train = create_sequences(train_scaled, sequence_length)\n",
    "    # Generate test sequences\n",
    "    X_test, y_test = create_sequences(test_scaled, sequence_length)\n",
    "    \n",
    "    print(f\"Training sequences shape: {X_train.shape}\")\n",
    "    print(f\"Test sequences shape: {X_test.shape}\")\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, scalers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lstm_model(input_shape):\n",
    "    \"\"\"\n",
    "    Build LSTM model architecture\n",
    "    \n",
    "    Parameters:\n",
    "    input_shape (tuple): Shape of input data (sequence_length, features)\n",
    "    \n",
    "    Returns:\n",
    "    model: Compiled LSTM model\n",
    "    \"\"\"\n",
    "    model = Sequential()\n",
    "    \n",
    "    # First LSTM layer with return sequences for stacking\n",
    "    model.add(LSTM(units=50, return_sequences=True, input_shape=input_shape))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Second LSTM layer\n",
    "    model.add(LSTM(units=50, return_sequences=False))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    # Dense layers for prediction\n",
    "    model.add(Dense(units=25))\n",
    "    model.add(Dense(units=1))\n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm_model(X_train, y_train, X_test, y_test, epochs=50, batch_size=32):\n",
    "    \"\"\"\n",
    "    Train the LSTM model\n",
    "    \n",
    "    Parameters:\n",
    "    X_train, y_train, X_test, y_test: Training and test data\n",
    "    epochs (int): Number of training epochs\n",
    "    batch_size (int): Training batch size\n",
    "    \n",
    "    Returns:\n",
    "    model: Trained model\n",
    "    history: Training history\n",
    "    \"\"\"\n",
    "    print(\"\\n==== Training LSTM Model ====\")\n",
    "    \n",
    "    # Get input shape from training data\n",
    "    input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "    model = build_lstm_model(input_shape)\n",
    "    \n",
    "    # Early stopping to prevent overfitting\n",
    "    early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stop],\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_test, y_test, scalers, target_col='Close'):\n",
    "    \"\"\"\n",
    "    Evaluate the LSTM model performance\n",
    "    \n",
    "    Parameters:\n",
    "    model: Trained LSTM model\n",
    "    X_train, y_train, X_test, y_test: Training and test data\n",
    "    scalers (dict): Dictionary of scalers for each feature\n",
    "    target_col (str): Target column name\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (train_predictions, test_predictions, metrics)\n",
    "    \"\"\"\n",
    "    print(\"\\n==== Evaluating Model Performance ====\")\n",
    "    \n",
    "    # Get target scaler for inverse transformation\n",
    "    scaler = scalers[target_col]\n",
    "    \n",
    "    # Make predictions\n",
    "    train_predictions_scaled = model.predict(X_train)\n",
    "    test_predictions_scaled = model.predict(X_test)\n",
    "    \n",
    "    # Inverse transform predictions and actual values to original scale\n",
    "    train_predictions = scaler.inverse_transform(train_predictions_scaled)\n",
    "    test_predictions = scaler.inverse_transform(test_predictions_scaled)\n",
    "    \n",
    "    # Transform the original y values back to original scale\n",
    "    y_train_orig = scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "    y_test_orig = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "    \n",
    "    # Calculate metrics - make sure arrays are the same length\n",
    "    train_rmse = math.sqrt(mean_squared_error(y_train_orig, train_predictions))\n",
    "    test_rmse = math.sqrt(mean_squared_error(y_test_orig, test_predictions))\n",
    "    \n",
    "    train_mae = mean_absolute_error(y_train_orig, train_predictions)\n",
    "    test_mae = mean_absolute_error(y_test_orig, test_predictions)\n",
    "    \n",
    "    train_r2 = r2_score(y_train_orig, train_predictions)\n",
    "    test_r2 = r2_score(y_test_orig, test_predictions)\n",
    "    \n",
    "    # Store metrics in dictionary\n",
    "    metrics = {\n",
    "        'train_rmse': train_rmse,\n",
    "        'test_rmse': test_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'test_mae': test_mae,\n",
    "        'train_r2': train_r2,\n",
    "        'test_r2': test_r2\n",
    "    }\n",
    "    \n",
    "    # Print metrics\n",
    "    print(f\"Training RMSE: {train_rmse:.2f}\")\n",
    "    print(f\"Test RMSE: {test_rmse:.2f}\")\n",
    "    print(f\"Training MAE: {train_mae:.2f}\")\n",
    "    print(f\"Test MAE: {test_mae:.2f}\")\n",
    "    print(f\"Training R²: {train_r2:.4f}\")\n",
    "    print(f\"Test R²: {test_r2:.4f}\")\n",
    "    \n",
    "    return train_predictions, test_predictions, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(df, train_predictions, test_predictions, sequence_length=60, target_col='Close'):\n",
    "    \"\"\"\n",
    "    Visualize model predictions against actual prices\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Original dataframe\n",
    "    train_predictions, test_predictions: Model predictions\n",
    "    sequence_length (int): Sequence length used for LSTM\n",
    "    target_col (str): Target column name\n",
    "    \"\"\"\n",
    "    print(\"\\n==== Visualizing Predictions ====\")\n",
    "    \n",
    "    plt.figure(figsize=(16, 8))\n",
    "    \n",
    "    # Plot actual data\n",
    "    plt.plot(df.index, df[target_col], color='blue', label=f'Actual {target_col} Price')\n",
    "    \n",
    "    # Calculate indices for predictions\n",
    "    train_size = int(len(df) * 0.7)\n",
    "    \n",
    "    # For training predictions: need to offset by sequence_length\n",
    "    train_start_idx = sequence_length\n",
    "    train_end_idx = train_size\n",
    "    train_plot_indices = df.index[train_start_idx:train_end_idx]\n",
    "    \n",
    "    # For test predictions: need to start after train data\n",
    "    test_start_idx = train_size + sequence_length\n",
    "    test_end_idx = test_start_idx + len(test_predictions)\n",
    "    \n",
    "    # Make sure test_end_idx doesn't exceed dataframe length\n",
    "    test_end_idx = min(test_end_idx, len(df))\n",
    "    test_predictions = test_predictions[:test_end_idx - test_start_idx]  # Adjust predictions length\n",
    "    \n",
    "    test_plot_indices = df.index[test_start_idx:test_end_idx]\n",
    "    \n",
    "    # Plot predictions - only if we have valid indices\n",
    "    if len(train_plot_indices) > 0 and len(train_predictions) > 0:\n",
    "        # Make sure lengths match\n",
    "        min_train_len = min(len(train_plot_indices), len(train_predictions))\n",
    "        plt.plot(train_plot_indices[:min_train_len], train_predictions[:min_train_len].flatten(), \n",
    "                 color='green', label=f'Training Predictions')\n",
    "    \n",
    "    if len(test_plot_indices) > 0 and len(test_predictions) > 0:\n",
    "        # Make sure lengths match\n",
    "        min_test_len = min(len(test_plot_indices), len(test_predictions))\n",
    "        plt.plot(test_plot_indices[:min_test_len], test_predictions[:min_test_len].flatten(), \n",
    "                 color='red', label=f'Test Predictions')\n",
    "    \n",
    "    plt.title(f'NVIDIA {target_col} Price Prediction')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel(f'{target_col} Price ($)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.savefig('nvidia_stock_prediction.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot loss during training if history is available\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(history.history['loss'], label='Training Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.title('Model Loss During Training')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss (MSE)')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        \n",
    "        plt.savefig('nvidia_training_loss.png')\n",
    "        plt.show()\n",
    "    except NameError:\n",
    "        print(\"Training history not available for plotting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_lstm(df, target_col='Close', sequence_length=60, epochs=50, batch_size=32):\n",
    "    \"\"\"\n",
    "    Main function to run LSTM prediction on stock data\n",
    "    \n",
    "    Parameters:\n",
    "    df (DataFrame): Processed dataframe with features\n",
    "    target_col (str): Column to predict (default: 'Close')\n",
    "    sequence_length (int): Number of time steps for sequences (default: 60)\n",
    "    epochs (int): Maximum training epochs (default: 50)\n",
    "    batch_size (int): Training batch size (default: 32)\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (model, metrics, predictions)\n",
    "    \"\"\"\n",
    "    # Prepare data\n",
    "    X_train, y_train, X_test, y_test, scalers = prepare_lstm_data(\n",
    "        df, target_col=target_col, sequence_length=sequence_length\n",
    "    )\n",
    "    \n",
    "    # Train model\n",
    "    global model, history\n",
    "    model, history = train_lstm_model(\n",
    "        X_train, y_train, X_test, y_test, \n",
    "        epochs=epochs, batch_size=batch_size\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    train_predictions, test_predictions, metrics = evaluate_model(\n",
    "        model, X_train, y_train, X_test, y_test, scalers, target_col\n",
    "    )\n",
    "    \n",
    "    # Visualize results\n",
    "    visualize_predictions(df, train_predictions, test_predictions, sequence_length, target_col)\n",
    "    \n",
    "    # Save model if needed\n",
    "    model.save('nvidia_lstm_model.h5')\n",
    "    print(\"Model saved as 'nvidia_lstm_model.h5'\")\n",
    "    \n",
    "    return model, metrics, test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Preparing Data for LSTM Model ====\n",
      "Using 31 features: ['calendarYear', 'revenue', 'grossProfit', 'netIncome', 'operatingIncome']...\n",
      "Train data shape: (889, 31), Test data shape: (381, 31)\n",
      "Training sequences shape: (829, 60, 31)\n",
      "Test sequences shape: (321, 60, 31)\n",
      "\n",
      "==== Training LSTM Model ====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">16,400</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">60</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">20,200</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,275</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">26</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │        \u001b[38;5;34m16,400\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m60\u001b[0m, \u001b[38;5;34m50\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │        \u001b[38;5;34m20,200\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25\u001b[0m)             │         \u001b[38;5;34m1,275\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │            \u001b[38;5;34m26\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,901</span> (148.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m37,901\u001b[0m (148.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">37,901</span> (148.05 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m37,901\u001b[0m (148.05 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 37ms/step - loss: 0.0474 - val_loss: 3.2234\n",
      "Epoch 2/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0111 - val_loss: 2.9550\n",
      "Epoch 3/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0071 - val_loss: 2.6299\n",
      "Epoch 4/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0047 - val_loss: 2.4958\n",
      "Epoch 5/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0050 - val_loss: 2.6349\n",
      "Epoch 6/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0038 - val_loss: 2.3123\n",
      "Epoch 7/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0037 - val_loss: 2.3472\n",
      "Epoch 8/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0031 - val_loss: 2.4101\n",
      "Epoch 9/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0026 - val_loss: 2.4063\n",
      "Epoch 10/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0025 - val_loss: 2.2842\n",
      "Epoch 11/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0021 - val_loss: 2.2199\n",
      "Epoch 12/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0024 - val_loss: 2.1593\n",
      "Epoch 13/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0025 - val_loss: 2.1358\n",
      "Epoch 14/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0025 - val_loss: 2.1002\n",
      "Epoch 15/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0024 - val_loss: 2.1867\n",
      "Epoch 16/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0021 - val_loss: 1.9930\n",
      "Epoch 17/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0020 - val_loss: 2.2361\n",
      "Epoch 18/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0023 - val_loss: 2.0619\n",
      "Epoch 19/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0018 - val_loss: 1.9750\n",
      "Epoch 20/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0018 - val_loss: 2.1620\n",
      "Epoch 21/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - loss: 0.0019 - val_loss: 2.0040\n",
      "Epoch 22/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - loss: 0.0015 - val_loss: 2.0079\n",
      "Epoch 23/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0016 - val_loss: 2.1397\n",
      "Epoch 24/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0015 - val_loss: 2.0451\n",
      "Epoch 25/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0014 - val_loss: 2.0676\n",
      "Epoch 26/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0014 - val_loss: 2.0656\n",
      "Epoch 27/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0017 - val_loss: 1.9708\n",
      "Epoch 28/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0013 - val_loss: 1.9366\n",
      "Epoch 29/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0013 - val_loss: 1.9158\n",
      "Epoch 30/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0015 - val_loss: 1.9595\n",
      "Epoch 31/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0016 - val_loss: 2.0898\n",
      "Epoch 32/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0014 - val_loss: 1.9933\n",
      "Epoch 33/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0013 - val_loss: 1.9517\n",
      "Epoch 34/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0014 - val_loss: 1.9650\n",
      "Epoch 35/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0012 - val_loss: 2.0241\n",
      "Epoch 36/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0013 - val_loss: 1.9206\n",
      "Epoch 37/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - loss: 0.0013 - val_loss: 2.0545\n",
      "Epoch 38/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - loss: 0.0012 - val_loss: 2.0584\n",
      "Epoch 39/50\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - loss: 0.0013 - val_loss: 1.9572\n",
      "\n",
      "==== Evaluating Model Performance ====\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [321, 829]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m enhanced_df\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Run LSTM prediction\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m model, metrics, predictions \u001b[38;5;241m=\u001b[39m \u001b[43mpredict_lstm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43menhanced_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mClose\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43msequence_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m60\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# 60 days of history\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\n\u001b[0;32m     20\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m==== Prediction Complete ====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel metrics: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmetrics\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36mpredict_lstm\u001b[1;34m(df, target_col, sequence_length, epochs, batch_size)\u001b[0m\n\u001b[0;32m     22\u001b[0m model, history \u001b[38;5;241m=\u001b[39m train_lstm_model(\n\u001b[0;32m     23\u001b[0m     X_train, y_train, X_test, y_test, \n\u001b[0;32m     24\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mepochs, batch_size\u001b[38;5;241m=\u001b[39mbatch_size\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Evaluate model\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m train_predictions, test_predictions, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscalers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_col\u001b[49m\n\u001b[0;32m     30\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Visualize results\u001b[39;00m\n\u001b[0;32m     33\u001b[0m visualize_predictions(df, train_predictions, test_predictions, target_col)\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mevaluate_model\u001b[1;34m(model, X_train, y_train, X_test, y_test, scalers, target_col)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Calculate metrics\u001b[39;00m\n\u001b[0;32m     31\u001b[0m train_rmse \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(mean_squared_error(train_predictions, test_predictions))\n\u001b[1;32m---> 32\u001b[0m test_rmse \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test_orig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_predictions_orig\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     34\u001b[0m train_mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(train_predictions, test_predictions)\n\u001b[0;32m     35\u001b[0m test_mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_test_orig, test_predictions_orig)\n",
      "File \u001b[1;32mc:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_regression.py:442\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean_squared_error\u001b[39m(\n\u001b[0;32m    383\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, multioutput\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform_average\u001b[39m\u001b[38;5;124m\"\u001b[39m, squared\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    384\u001b[0m ):\n\u001b[0;32m    385\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Mean squared error regression loss.\u001b[39;00m\n\u001b[0;32m    386\u001b[0m \n\u001b[0;32m    387\u001b[0m \u001b[38;5;124;03m    Read more in the :ref:`User Guide <mean_squared_error>`.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;124;03m    0.825...\u001b[39;00m\n\u001b[0;32m    441\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 442\u001b[0m     y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[0;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    445\u001b[0m     check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    446\u001b[0m     output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[1;32mc:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_regression.py:100\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[1;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_reg_targets\u001b[39m(y_true, y_pred, multioutput, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumeric\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     67\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same regression task.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;124;03m        correct keyword.\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 100\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    101\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    102\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m check_array(y_pred, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n",
      "File \u001b[1;32mc:\\Users\\lukis\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\utils\\validation.py:387\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    385\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    386\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 387\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    388\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    389\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    390\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [321, 829]"
     ]
    }
   ],
   "source": [
    "# Example usage with the processed dataframe from your previous code\n",
    "if __name__ == \"__main__\":\n",
    "    # Assuming 'enhanced_df' is the output from your previous processing\n",
    "    # You can load it from the saved CSV or use the result from the previous function\n",
    "    enhanced_df = pd.read_csv(\"nvda_merged_dataset.csv\")\n",
    "    \n",
    "    # Convert Date back to datetime\n",
    "    enhanced_df['Date'] = pd.to_datetime(enhanced_df['Date'])\n",
    "    \n",
    "    # Set Date as index for easier time series analysis\n",
    "    enhanced_df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Run LSTM prediction\n",
    "    model, metrics, predictions = predict_lstm(\n",
    "        enhanced_df, \n",
    "        target_col='Close',\n",
    "        sequence_length=60,  # 60 days of history\n",
    "        epochs=50,\n",
    "        batch_size=32\n",
    "    )\n",
    "    \n",
    "    print(\"\\n==== Prediction Complete ====\")\n",
    "    print(f\"Model metrics: {metrics}\")\n",
    "    print(f\"Test predictions shape: {predictions.shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
